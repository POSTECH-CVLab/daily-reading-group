# 2021 July
## Full List of papers - updated weekly

* Baking Neural Radiance Fields for Real-Time View Synthesis ([진오](./summary/1.md))
* NeRF: Representation Scenes as Neural Radiance Fields for View synthesis ([민국](./summary/2.md))
* A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications ([우현](./summary/3.md))
* PixelNerf: Neural Radiance Fields from One or Few Images ([민국](./summary/4.md))
* Hypercorrelation Squeeze for Few-Shot Segmentation ([승욱](./summary/5.md))
* Dense Contrastive Learning for Self-Supervised Visual Pre-Training ([우현](./summary/6.md))
* Unsupervised Learning of Dense Visual Representations ([우현](./summary/7.md))
* Neural Reprojection Error: Merging Feature Learning and Camera Pose Estimation ([승욱](./summary/8.md))
* Few-shot Image Generation via Cross-domain Correspondence ([우현](./summary/9.md))
* EfficientDet: Scalable and Efficient Object Detection ([승욱](./summary/10.md))
* Depth-supervised NeRF: Fewer Views and Faster Training for Free ([진오](./summary/11.md))
* Occupancy Networks - Learning 3D Reconstruction in Function Space ([진오](./summary/12.md))
* Rethinking and Improving the Robustness of Image Style Transfer ([우현](./summary/13.md), [승욱](./summary/33.md))
* RepVGG: Making VGG-style ConvNets Great Again ([승욱](./summary/14.md))
* ViTGAN: Training GANs with Vision Transformers ([민국](./summary/15.md))
* GIRAFFE: Representing Scene As Compositional Generative Nerual Feature Fields ([윤우](./summary/16.md), [민국](./summary/25.md), [수현](./summary/67.md))
* kiloNeRF ([윤우]](./summary/17.md))
* TransGAN: Two Pure Transformers Can Make One strong GAN, and That Can Scale Up ([민국](./summary/18.md))
* Per-Pixel Classification is Not All You Need for Semantic Segmentation ([우현](./summary/19.md))
* Residual Network Behave like ensembles of Relatively Shallow Networks ([승욱](./summary/20.md))
* D-NeRF: Neural Radiance Fields for Dynamic Scenes ([진오](./summary/21.md))
* SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition ([승욱](./summary/22.md))
* MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators ([우현](./summary/23.md))
* NeRF--: Neural Radiance Fields Without Known Camera Parameters ([진오](./summary/24.md))
* Stereo radiance fields ([윤우]](./summary/26.md))
* Weakly-supervised physically Unconstrained Gaze Estimation ([승욱](./summary/27.md))
* Convolutional Occupancy Networks ([윤우](./summary/28.md))
* Generative Multi-Adversarial Networks ([우현](./summary/29.md))
* GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis ([민국](./summary/30.md), [진오](./summary/31.md))
* Editing Conditional Radiance Fields ([진오](./summary/32.md), [민국](./summary/42.md))
* Swapping Autoencoder for Deep Image Manipulation ([우현](./summary/35.md))
* divco diverse conditional image synthesis via contrastive generative adversarial network ([민국](./summary/36.md))
* Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments ([강희](./summary/37.md))
* ChannelPruning for Accelerating Very Deep Neural Networks ([승욱](./summary/38.md))
* CorrNet3D : Unsupervised End-to-end Learning of Dense Correspondence for 3D point clouds ([강희](./summary/39.md))
* On the Continuity Rotation Representations in Neural Networks ([윤우](./summary/40.md))
* Anycost GANs for Interactive Image Synthesis and Editing ([우현](./summary/41.md))
* Indoor Visual Localization with Dense Mathing and View Synthesis ([강희](./summary/43.md))
* NERF Research Directions ([윤우](./summary/44.md))
* Taming Transformers for High-Resolution Image Synthesis ([우현](./summary/45.md))
* ShaRF: Shape-conditioned Radiance Fiedls from a Single View ([진오](./summary/46.md))
* Learning Deep Features for Discriminative Localization ([승욱](./summary/47.md))
* cGANs with Auxiliary Discriminative Classifier ([민국](./summary/48.md))
* Nerfies: Deformable Nerual Radiance Fields ([진오](./summary/49.md))
* Correlated Input-Dependent Label Noise in Large-scale Image Classification ([승욱](./summary/50.md))
* Playable Video Generation ([우현](./summary/51.md))
* Few-shot Image Generation via Cross-domain Correspondence ([우현](./summary/52.md))
* A Simple Framework for Contrastive Learning of Visual Representations ([수현](./summary/53.md))
* On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation ([민국](./summary/54.md))
* MinkLoc++ : Lidar and Monocular Image Fusion for Place Recognition ([강희](./summary/55.md))
* KeypointDeformer: Unsupervised 3D Keypoint DIscovery for Shape Control ([승욱](./summary/56.md))
* High-Fidelity Neural Human Motion Transfer from Monocular Video ([우현](./summary/57.md))
* Big self-supervised Models are Strong Semi-supervised learners ([수현](./summary/58.md))
* NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video ([우현](./summary/59.md))
* Quantifying Attention Flow in Transformers ([승욱](./summary/60.md))
* Repurposing GANs for One-Shot Semantic Part Segmentation ([우현](./summary/61.md), [민국](./summary/63.md))
* PIC-NET : Point Cloud and Image Collaboration Network for Large-Scale Place Recognition ([강희](./summary/62.md))
* Training Generative Adversarial Networks in One Stage ([민국](./summary/64.md))
* Transformer Interpretability Beyond Attention Visualization ([승욱](./summary/65.md))
* GNeRF: GAN-based Neural Radiance Field without Posed Camera ([진오](./summary/66.md))
* Exploring Simple Siamese Representation Learning ([수현](./summary/68.md))
* Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers ([승욱](./summary/69.md))
* Nex: Real-time View Synthesis with Neural Basis Expansion  ([우현](./summary/70.md))
