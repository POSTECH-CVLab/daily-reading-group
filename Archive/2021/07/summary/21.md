# D-NeRF: Neural Radiance Fields for Dynamic Scenes
### Author information - CVPR 2021
#### Summarized by Jinoh Cho

```
기존의 NeRF는 Static Scene에서만 적용이 가능했습니다. 하지만 이번 논문에서 제안하는 D-NeRF는 Dynamic Scene에서 적용 가능한 NeRF를 제안하게 됩니다.

이번 논문이 다루는 문제 상황은 다음과 같습니다. Non-rigid하게 움직이는 Dynamic Scene을 여러 각도에서 카메라로 사진을 찍고 해당 사진이 학습 데이터로 주어지게 됩니다. 그리고 우리는 딥러닝 모델이 이 Scene을 Implicit하게 encode 할 수 있도록 해야 하고 Novel View가 주어졌을 때 잘 합성할 수 있도록 하게 하면 됩니다. 

기존의 NeRF는 Sampling Point의 3D coordinate 좌표와 ray의 방향을 나타네는 theta, phi를 input으로 받아서 해당 Sampling Point에서의 RGB 색정보와 Volume Density를 Output으로 내보내는 Continuous Scene Representation을 학습하게 됩니다. 이 논문에서 제안하는 논문은 기존 NeRF의 인풋에 추가로 시간 정보가 주어지게 됩니다. 따라서 6D -> 2D function을 학습한다고 보면 됩니다. 

가장 간단한 방법으로는 단순히 샘플링 점의 좌표값 그리고 viewing direction 그리고 시간이 인풋으로 주어졌을때 색정보와 volume density를 출력하는 함수를 학습하는 방법이 있을 수 있습니다.
 
허나 이 논문에서는 두가지 함수로 나누어서 학습을 하게 됩니다. 첫번째 함수는 현재 시간 t의 샘플링 점을 t=0일 때인 canonical configuration에서의 점으로 매핑을 해주는 함수. 그리고 두번째 함수는 canonical configuration에서의 scene reperesentation을 학습하는 함수 입니다. 
 
이 두 가지 함수로 나누어서 학습을 했을 때가 통일된 함수 하나로 학습하는 것보다 훨씬더 좋은 성능을 보여주는 것을 실험 결과를 통해서 보여주게 됩니다.
 
이 논문을 읽는 데에 특별히 어려운 점은 없이 쉽게 이해가 되도록 설명 해 놓은 듯 합니다. 이 논문의 가장 큰 커트리뷰션이라고 하면 이 논문에서 제안하는 방법이 Dynamic Scene에서 처음 NeRF 기법을 적용 했다는 점인 것 같습니다.
```