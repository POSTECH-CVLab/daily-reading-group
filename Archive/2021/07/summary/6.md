# Dense Contrastive Learning for Self-Supervised Visual Pre-Training
### Xinlong Wang et al., University of Adelaide (CVPR 2021 Oral)
#### Summarized by Woohyeon Sim

* **global feature와 함께 local feature의 dense correspondence를 self-supervised learning에** 이용한 논문. backbone feature로 correlation tensor를 구한뒤 max가 되는 pair로 positive 형성. 즉, positive는 local feature로 만들고 negative는 pooled feature로 들어가는 식임. 아키텍쳐는 projection head를 dense와 global을 구분하지만 마지막 global feature 만드는 것 빼고는 정확히 똑같이 구성함. 그리고 이 둘은 각각 따로 학습하는데 global loss는 기존과 동일하며, dense loss는 correspondence를 수행한 backbone이 아닌 projection head에서 나오는 local feature로 모든 위치에 구해서 합함.
* **강점**: augmentation 한 것과 correspondence를 구하기 때문에 매칭되는 점이 무조건 존재하므로 아이디어가 유효함. 오버헤드가 적음 (에폭당 계산시간이 몇 초 차이라고 함). dense prediction task에 transfer 할때의 성능 향상 폭이 큼 (신기했던건 imagenet pretrained로 한 것보다 높게 나옴. 저자는 이것을 image-level prediction과 pixel level prediction의 discrepancy가 있기 때문으로 설명).
* **약점**: dense contrastive learning만으론 성능이 현저히 떨어짐. 매칭 그리드가 7x7로 아주 작음. 매칭 결과가 좋은 것 같지 않음.
* **총평**: 단순한 아이디어로 큰 효과를 냈음. 그러나 매칭 성능을 볼 때 여전히 개선의 여지가 많은 금광.
