# Generative Multi-Adversarial Networks
### Ishan Durugkar et al., Univ. of Massachusetts - ICLR 2017
#### Summarized by Woohyeon Sim

**Task**: Developing GANs with multiple discriminators
	
**Main novelty**: Exploring discriminator's role ranging between two extremes, formidable adversary and forgiving teacher
	
**Formidable adversary (X)**: train generator against the best discriminator
* forces generator to synthesize high fidelity samples that hold up the scrutiny of all N discriminators (here, each discriminator does not cooperate with others).
		
* max D is not performed to convergence (or global optimality) in practice and fails to produce promising results on the image generation task, which motivates the next section.
	
**Forgiving teacher (O)**: train soft-version discriminator
	
* automatically temper the performance of the discriminator, but still encourages the generator to challenge itself against more accurate adversaries.
		
* use Pythagorean means (See paper) instead of max function, which parametrized by λ so as to mean function corresponds to λ = 0 and the max is recovered as λ → ∞.
		
* set λ closer to zero at the beginning of training and gradually increase λ as training progresses to become more critical for more refined training. (achieved by simply incentivizing generator for larger λ with 0.001λ at the loss)
	
	
	
**Experiments with MNIST**: the method stablizes and accerlerates the training and achives better performance than GANs with a single discriminator.
	
**총평**: 아주 초기 논문이란 느낌이 물씬 드는 논문. 접근 방식이 그래도 재미있었음. 이 논문은 discriminator 사이의 output의 correlation을 고려하지 않는 반면 앞서 요약한 MCL-GAN은 그들 사이를 명확히 구분한 상태로 학습하여 성능을 높임. 즉, MCL-GAN은 여기서 말한 formidable adversary개념을 구현하는 데 성공하여 성능을 높인 것에 차별점이 있음.