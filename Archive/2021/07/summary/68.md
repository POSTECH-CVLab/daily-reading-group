# Exploring Simple Siamese Representation Learning
### Author Information - CVPR 2021) - best paper candidate
#### Summarized by Suhyun Jung
---

**Overview**: 1) negative sample pair 2) large batches 3) momentum encoders 이 세 가지가 하나도 없어도 simple Siamese network 가 충분히 의미 있는 representation 을 잘 학습할 수 있다는 것을 보였다.

 

**task** : feature representation (?) 

 

**Motivation** : Siamese based Method 는 good representation 을 학습하기 위해 많이 사용되어 왔음.
이들은 안정적인 학습을 위해 큰 batch size 를 요구하거나 collapsing 을 방지하기 위해 momentum encoder, negative pairs, clustering 등을 필요로 하였음. 하지만 그 어느 것도 필요로 하지 않고도 잘 학습될 수 있음을 본 논문에서 제시한 간단한 Siamese based architecture, SimSiam 을 통해 보이고자 함.

 

**Method** : 아키텍쳐는 SimCLR without negative pairs / SwAV without online clustering 과 유사함. 
1) 동일한 image 로부터 augment 된 x1, x2 를 사용, 
2) same encoder f 를 사용해 두 encoded feature z1,z2 를 얻고 
3) 한쪽만 MLP predictor h 를 통과시켜 feature p 를 얻는다. 
4) feature p 와 predictor h 를 통과하지 않은 encoded feature z 사이 negative consine similarity 를 계산하여 loss 로 두는데, 이 때  predictor h 를 통과하지 않은 encoded feature z 는 상수 취급하여 gradient 를 계산하지 않는다(이렇게 한쪽만 grad를 계산하지 않는 것을 stop-gradient 라고 한다.). 이렇게 z1,z2 를 각각 h 에 통과시킨 두 negative cosine similarity 를 평균한다. 전체 loss 는 minibatch 의 모든 image 에 대해 이 과정을 적용하여 평균을 구한 것이다. loss 의 minimum possible value 는 -1 이다. 

 

**Experiment** ; 저자는 method 의 각 요소 중 무엇이 학습에 중요하게 작용했는지 알아보기 위해 다양한 실험을 진행하였다. 간략하게 요약하자면, 
1) stop-gradient 가 collapse 를 막는 가장 핵심적인 요소였다. 동일한 아키텍쳐에서 stop-gradient 만 뺀 경우, collapsing 이 일어나는 것을 확인할 수 있었다. 
2) predictor h 의 존재 역시 중요했다. predictor h 가 존재하지 않으면 ( same as identity mapping) loss 는 stop-grad 를 적용하지 않은 경우와 똑같게 되기 때문이다(정확히는 1/2). 이는 현재 사용하는 loss 가 symmetric 이기 때문인데, assymetric 인 경우에도 h 를 없애면 학습이 잘 되지 않는 것을 관측했다고 한다. 
3) 저자가 제안한 SimSiam 모델은 다른 Siamese based model 들과 달리 large batch size 가 아니더라도 거의 비슷한 성능을 낼 수 있다는 것을 실험으로 확인하였다. (최소 64 batch size 의 acc 가 최대 acc 와 비교했을 때 2 % 정도 밖에 차이가 나지 않았다. (66.1, 68.1))
4) batch normalization 은 적절히 사용할 시 학습을 도우긴 했지만 collapsing 을 방지해 주진 않았다. cross-entropy similarity 를 loss 에 사용 시 성능이 조금 떨어지긴 했지만 비슷한 acc를 보여, cosine similarity 가 collapsing 방지에 필수적인 건 아님을 보였다.

 

저자는 implication of stop-gradient 에 대한 hypothesis 를 설정하고, 이를 proof-of-concept experiment 로 검증하였다(논문 참조). 하지만 저자는 여전히 collapsing 을 방지하는 이유에 대해 설명하는 것이 아닌, empirical observation 에 불과하다고 comment 하였다. 

 

**총평** : writing 도 깔끔하게 잘 되어 있고, 요약하진 않았지만 hypothesis 에 대한 proof 나 다른 Siamese based method 들과의 relation 에 대한 저자의 생각들을 적어 놓아 궁금하신 분들은 한번 쯤 읽어 보기 좋은 논문인 것 같다.