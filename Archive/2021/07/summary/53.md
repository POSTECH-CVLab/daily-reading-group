# A Simple Framework for Contrastive Learning of Visual Representations
### Author Information
#### Summarized by Soohyun Jung
---

augmentation 을 이용한 contrastive learning 을 제안한 논문입니다. 성능을 위한 최적의 augmentation, loss 및 design detail 에 관한 실험 결과 및 고찰을 포함하고 있습니다. 

**Method** \
**Overall :** base encoder 와 projection head 로 구성된 network 이면 적용이 가능한 method 로, two separate data augmentation operator 을 동일한 image x 에 각기 적용한 후, 이를 positive sample 로 하여 contrastive learning 을 한 것이다. negative sample 의 경우 minibatch 의 다른 image 로부터 생성된 augmented result 를 전부 negative 로 두고 학습하였다.
논문에서는 Resnet-50과 2 layer MLP 를 각각 encoder 와 head 로 두었다. 

**Global BN :** 모델이 prediction accuracy 를 높이기 위해 local information leakage 를 학습할 수 있어서 BN mean 과 variance 를 all devices 에 대해 aggregate 하여 문제를 해결하였다.

**Data augmentation for Contrastive Representation Learning :** random cropping 과 random color distortion 을 적용한 경우 accuracy 가 가장 높았다. 그 중 color distortion 을 적용하지 않을 경우 color histogram 만으로 image 를 구분할 수가 있어서 모델이 representation 을 제대로 학습하지 못하기 때문에 중요하다.

**Loss function :** 첨부된 식과 같은 loss 를 모든 positive pairs (both (i,j) and (j,i)) 에 대해 계산한다. 이 normalized temperature-scaled cross entropy loss 를 논문에서는 NT-Xent 라고 부른다. 실험을 통해 저자들은 다음을 관측했다.
1) l2 normalization + tenperature can effectively weights different examples
2) appropriate temperature help model learns from hard negatives
3) corss-entropy weigh the negatives by their relative hardness
위와 같은 이유로 인해 다른 contrastive loss 에도 l2 normalization 을 적용하고, cross-entropy 를 사용하지 않는 loss 를 위해 semi-hard negative mining 도 적용했으나, 여전히 NT-Xent 의 성능이 가장 좋았다.

**projection head :** nonlinear projection head를 사용해 training 하는 것이 linear projection head 나 no projection 보다 높은 성능을 보였다. 즉, head 직전 hidden layer 의 representation 이 가장 우수했다. 이는 head 를 통과한 representation 과 직전 representation 사이에도 마찬가지인데, projection head를 통과하게 되면 contrastive task 에서는 필요 없지만 downstream task 에서는 필요할 수도 있는 information 을 제거하기 때문이다.

이외에도 논문에서는 batch size 가 커질수록, model size 가 클수록 contrastive learning 의 benefit 이 커지는 결과를 보여 주었다.

**총평 :** 들어보기만 하고 보지는 않았던 simCLR 논문을 읽어 보았습니다. augmentation 이 strong 할수록 성능 향상을 보였는데, 제시된 augmentation 을 2개 이상 시퀀셜하게 적용한 결과는 어떨지 궁금하네요
