# RepVGG: Making VGG-style ConvNets Great Again
### Xiaohan Ding et al., BNRist, Tsinghua U, HKUST - CVPR 2021
#### Summarized by Seungwook Kim

```
Task: Image Classification (downstream task: Semantic segmentation)

Main novelty/Contribution: 
VGG-style 네트워크들은 보다 심플한 디자인을 띄고 있다. 
오직 3x3 conv와 ReLU operator만 이루어져 있기에 
Flop수치로는 비교적 높을지언정 MAC과 parallelism을 고려하면 (Winograd convolution) 오히려 더 빠른 속도를 보이기도 한다.
 
그러나 요새 다양한 Network들은 residual connection과, 이에 따라서 channel dim 을 맞추기 위한  1x1 convolution이 포함되어있다.
이런 multi-branch design은 확실히 성능적 이점이 있으나, inference때 단순 3x3 conv로만 이루어져있는 네트워크에 비해 느리고 memory utilization 또한 떨어진다. 
 
이 논문은 multi-branch 학습의 성능적 이점과, plain convolution network의 inference time speed 이점을 둘 다 챙기는 논문이다.
이를 위해 structured re-parametrization을 사용한다.
 
train time때는 실제로 residual connection / 1x1 convolution / Batch normalization 을 사용하여 train을 하고,
inference time 때는 학습한 1x1 convolution / Batch normalization등의 학습된 weight들을 structured re-parametrization을 통해
동일한 결과를 내는 네트워크이지만, 3x3 conv와 ReLU로만 이루어져있는 모델로 바꾼다.
(논문의 그림을 보면 쉽게 이해할 수 있음)
 
크기가 비슷하거나 더 큰 모델들에 비해 뛰어난 성능을 내며, 가장 주목할 점은 속도가 빠르다는 점이다.
FLOP이 비교적 높아도 Plain Convolutional Network의 속도가 빠를 수 있는 것은 요새 Hardware들이 지원해주는 Acceleration 덕분이다.
(Multi-branch network의 경우 많이 사용하기 어려운 acceleration이라고 함)
Downstream task인 semantic segmentation에도 좋은 성능을 보임

```

총평:


	
* Flop이 속도에 가장 중요한 것은 아님 (MAC, Degree of parallelism 또한 중요)



	
* 1x1 conv, BN, residual connection을 사용하여 학습한 network를
	structured reparametrization으로 쉽게 3x3 conv 기반만으로 네트워크를 만들수 있음 
	(같은 성능,  속도 이점) --> 코드 공유 되어있음
	
* 다만 hardware acceleration이 제한되어있는 mobile 등의 상황에서는 별로 gain이 없을 수 있음
	
* 다양한 multi-branch architecture를, inference time때만 acceleration이 가능하며 간단한 convolution 기반의 네트워크로 바꿀 수 있다는 점이 가장 인상적