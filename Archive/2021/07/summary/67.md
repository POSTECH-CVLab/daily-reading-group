# GIRAFFE : Representing Scenes as Compositional Generative Neural Feature Fields
### Author Information - CVPR 2021 Best Paper
#### Summarized by Suhyun Jung
---

**Overview** : photo-realistic controllable 3D scene 을 생성하는 novel method, GIRAFFE 를 제안함.

 

**task** : image synthesis

 

**Motivation** : real application 에는  high resolution photorealistic image synthesis 뿐만 아니라 이를 control 하는 것이 요구되는 경우가 많다. controllable 한 image synthesis 를 연구한 최신의 논문들도 3D 가 아닌 2D operation 을 control 한 것이 대부분이다. real world 는 3D 인 만큼, controllable 3D scene 의 중요성이 매우 커 이러한 방법을 고안하고자 함.

 

**Method** : 
다음과 같은 두 insight 에 기반해 method 를 디자인하였다. 
1) incorporating a compositional 3D scene representation directly into the generative model leads to more controllable image synthesis
2) combining this explicit 3D representation with a neural rendering popeline results in faster inference and more realistic images

 

Method 는 다음의 세 단계로 이루어진다. 
1) Objects as Neural Feature Fields : 각각의 entities 에 대해 scale, translation, rotation 에 대한 transformation, 그리고 appearance 와 shape code 를 이용해 canonical object space 상의 feature field (able to predict feature vector 와 density) 를 학습한다. 
2) Scene Composition ; point x 와 viewing direction d 에 대해 모든 entity 의 density 를 합하고, feature vector 의 경우 density-weight mean 으로 combine 한다. 
3) efficient combination of volume and neural rendering

 


**Limitation** : 
1) data 자체에 bias 가 있는 경우 variation 을 잘 학습하지 못하고 bias 에 크게 영향을 받는다 (ex. face dataset 에서 옆모습도 눈은 정면을 응시하고 있어서 rotation 을 적용해도 눈은 rotate 되지 않는 예시) 
2) camera pose/ object-level transformation 의 uniform distribution 과는 달리 real distribution 은 uniform 하지 않은데, real distribution 에 영향을 많이 받아 이상한 결과물을 때로 생성하기도 한다.

 

**총평** : 3D 분야를 아직 잘 모르기도 하고 NeRF 나 GRAF 를 제대로 읽어보지 않았는지라 아직 이해나 정리가 조금 부족하다고 생각됩니다. 아직 제대로 이해를 하지 못해 idea 나 method 에 대한 평가는 어렵지만, 전반적으로 best paper 답게 깔끔하게 writing 이 되어 있다는 느낌을 받았고, 3D scene controlling 의 초석을 깔았다는 점에서 높이 평가할 만한 논문인 것 같습니다.