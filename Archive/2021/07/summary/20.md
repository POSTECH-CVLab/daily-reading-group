# Residual Network Behave like ensembles of Relatively Shallow Networks
### Andrea Veit et al., Cornell University : NeurIPS 2016
#### Summarized by Seungwook Kim

**Task**: Residual networks의 새로운 해석 및 결론

**Main Findings:**
* Paths through residual networks vary in length (N개의 residual module이 있을 때, 단 하나의 residual module만 통과하는 path 또한 N개)
* Lesion study) These variable-length paths do not strongly depend on one another
    * Residual module을 몇 개 제거해도 성능이 눈에 띄게 하락하지 않는 모습
	* Single-path network (ex. VGG)에서는 중간 레이어 하나만 없애도 이후 layer들의 input distribution 이 모조리 바뀌게 돼서 성능이 박살남
* esion Study) These variable-length paths behave like emsembles 
	* path의 개수를 늘리고 줄임에 따라 성능이 smooth하게 오르고 떨어짐
	* Key characteristic of ensembles: Performance depends smoothly on the number of members
* 다양한 길이의 Path들 중, 비교적 짧은 Path들만이 학습에 기여한다 (gradient의 크기를 바탕으로 판단)
	* Residual Network를 사용하는 모듈의 깊이가 깊더라도, 실제로 training되고 있는 path들은 비교적 짧은 길이의 path들이라는 뜻
	* 예) 110 layer의 residual network 기반 모델에서, 학습시 대부분의 gradient는 깊이가 10-34정도 되는 path들에서 기인함
	실제로 해당 길이의 path들만을 학습했을 때 성능 하락이 없는 것을 확인 (오히려 간소하게 높아짐)
* Maximum path length 가 50정도인 경우 (50여개의 residual module), path길이가 5~17인 path들이 가장 training에 많이 기여하는 것을 확인 (전체 path의 0.5%정도밖에 되지 않는다고 함!)
	* 따라서 network 가 deep하더라도, residual network를 사용하면 effective path는 shallow 하다고 함
	* (Thus, ensemble of shallow networks)

ResNet이 잘된다고 생각했던 이유 중 하나가 "Residual connection을 통해 network를 degradation없이 깊게 만들 수 있어서"라고 생각했는데, 결론적으로 residual network를 사용하더라도 effective path들은 shallow하기 때문에, depth에 따른 vanishing gradient문제는 여전히 해결되지 않았다는 결론입니다.