#  Correlated Input-Dependent Label Noise in Large-scale Image Classification
### Mark Collier et al., Google AI - CVPR 2021
#### Summarized by Seungwook Kim
---

**Task**:

Image classification (considering label noise present in today's classification datasets)

**Contribution**: 
* Proposes a multivariate Normal distributed latent variable on the final FC layer of a neural network classifier.
    * the covariance matrix of this latent variable models the aleatoric uncertainty due to label noise.
* Demonstrates that the learned covariance structure captures known source of label noise between "semantically similar(appenzeller dog vs entlebucher dog)" and "co-occuring(keyboard vs spacebar)" classes.
* Efficient parameterization of covariance matrix
    * conventional Covariance matrix: O(DK^2)
    * Low-rank approximation: O(DKR), R << K
    * Parameter-efficient low-rank approximation, for datasets where number of classes is too large: O(DK + KR)
 
**Main points:**
* Supposes the generative process for labels, where a label is generated by sampling from the utility and taking the argmax.

> NOTE: The popular cross-entropy used in training neural network classification models is the closed form solution when the sthocastic component of the generative process is chosen to be a standard Gumbel.

> NOTE2: Therefore, the generative process with Gumbel noise distribution is already an implicit standard assumption when training classification models.

**윗부분 정리**: 기존의 classification model들에서 사용되는 일반적인 softmax cross-entropy model 또한 이미 label noise를 가정하고 있으며, 이 경우 standard Gumbel noise distribution을 가정하고 있다.
 
**문제점:** 이 경우 noise에 "independence"와 "identical" assumption이 부여되는데, 실제 상황에서 발생하는 노이즈들은 indepedent하거나 identical 하지 않다.
 
따라서 본 논문에서는 noise term을 distributed multivariate normal로 가정하여 independence와 identical 가정을 break한다.
다만 이 경우 closed form solution이 존재하지 않는데, 따라서
1) Monte Carlo expectation을 통해 근사하고
2) argmax -> softmax with parameterized temperature로 변경하여 gradient computation을 feasible하게 한다.}

여기서 latent variable의 covariance matrix를 efficient하게 나타내여 일반적인 low-rank approximation보다 parameter/computational requirements를 줄인다.
 
ablation 결과, 일반적인 diagnalized covariance matrix만 사용해도 성능이 꽤나 높아지나 이와 같이 (efficient form이긴 하지만)full covariance matrix를 쓰는 것이 실제로 성능에 도움이 많이 된다는 것을 보임.
WebVision1.0 (ImageNet과 같은 class 종류, crawl되어 noise가 많음)에서 SoTA,
ILSVRC12와 JFT 에서도 다른 label noise관련 work대비 SoTA
 
**총평:** Writing이 나름 깔끔하고, 수식도 나름 깔끔하게 정리되어 있어 읽기에 불편하진 않았습니다.
그러나 수학적인 부분을 완벽하게 이해하지 못해서 PRML을 공부하고 싶게 만드는 그런 논문이네요...
수학적인 부분에서 motivation을 이끌어내고, 이를 가능하게 하는 efficient한 구현이 main novelty로 작용하여 실제로 성능과 실험도 좋아서 oral paper로 뽑히게 된 것 같습니다.