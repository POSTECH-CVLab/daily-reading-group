# Transformer Interpretability Beyond Attention Visualization
### Hila Chefer et al., Tel Aviv University, FAIR - CVPR 2020
#### Summarized by Seungwook Kim
---

**Task**:  \
Relevancy for Transformer Networks (Interpreting transformers)
 
Relevancy: Relevancy scores, 관련도
 
**Motivation**: \
Transformer (with attention layers + skip connections) are not sufficiently represented by existing methods of interpretation.
 
**Existing methods**: 
1) Gradient-based methods : based on gradients w.r.t input of each layer, computed through backprop. \
--> These methods are mostly class agnostic: similar outputs are obtained regardless of the class used to compute the propagated gradient \
------> therefore limited classwise interpretation \
--> GradCAM is class-specific. but based only on the gradients of the deepest layers \
------> Therefore the obtained result by upsampling these low-spatial resolution is coarse
 
2) Attribution propagation methods: justified by Deep Taylor Decomposition framework (DTD) \
--> Decompose the decision made by the network into the contributions of the previous layers recursively down to the elements of the network's input \
--> Most commonly used: Layer-wise relevance propagation(LRP) method-> assumes that ReLU is used \
--> Transformers use different nonlinearity (GeLU), and negative values can exist - LRP is not directly applicable \
--> Many of these methods are class-agnostic in practice
 
3) Other categories
Saliency based methods, Activation maximization, Excitation Backpropagation
Pertubation methods: consider change to the decision of the network as small changes are applied to the input. \
--> Intuitive and applicable to blackbox models \
--> However, process of generating a heatmap is computationally expensive.
 
Attention rollout (어제 소개한 논문) : Assumes that attentions are combined linearly \
--> Actually: Attention is combined nonlinearly 
 
**Method:** \
LRP (Layerwise-relevance propagation) based. \
Incorporate both relevancy and gradient information, in a way that iteratively removes negative contributions (can be used for non-ReLU). \
**Result**: Class-specific visualization for self-attention modules
 
Method가 엄청 novel하다기보다, 기존의 방법론을 1) Non-RELU 를 처리할 수 있도록 변경, 2) Relevance가 유지되도록 (Relevance in == Relevance out) mathematical instability를 없애기 위해 normalize 정도의 "수정"을 거쳐서 transformer라는 특수한 architecture에 적용할 수 있도록 하였습니다. (Transformer는 CLS 토큰만을 사용하여 classification을 하기도 하니..)
 
**Experiments:** \
Qualitative하게 Class-specific Visualization이 잘 됨 \
Quantitative: Perbutation test에서 가장 좋은 성능 (중요하다고 visualize된 부분들이 실제로 더 중요했다는 뜻) \
Quantitative: ImageNet-segmentationㅔ서의 segmentation performance또한 다른 visualization method에 비해 SoTA
 
**총평:** \
Interpretability 분야에 친숙하지 않아서 gradient/relevance관련 수식에 익숙하지 않아서 완벽한 이해는 어려웠습니다. \
다만 이 논문에 contribution은 단순히 LRP (이 분야 근본 논문 중 하나로 보임) 수식을 조금 바꾸어 1) ReLU가 아닌 linearity 를 사용하는 architecture 2) Transformer architecture 에도 적용 가능하도록 수정했다는 부분에 novelty가 있으며, 이를 다양한 실험을 통해 증명했다는 것이 크게 작용하여 accept된 것 같습니다.
 
Related work가 상세히 잘 적혀 있는 것으로 보여, interpretability/explainability 분야에 관심이 있다면 해당 논문으로 시작하여 citation만 따라가도 탄탄한 background가 잡힐 것 같다는 생각이 들었습니다. 저는 하루동안만 읽는 거라 cited work까지 제대로 읽지는 못했지만..!