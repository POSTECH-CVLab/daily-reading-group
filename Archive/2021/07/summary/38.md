# ChannelPruning for Accelerating Very Deep Neural Networks
### Yihui He et al., Xi'an Jiaotong University - ICCV 2017
#### Summarized by Seungwook Kim

 
**Task**: Accelerating Deep Neural Network (CNN) : **Inference-time**
CNN Acceleration은 당시에 크게 optimized implementation (FFT), Quantization (BinaryNet), Structured simplification (Compact CNN)으로 나뉘었다. 해당 논문에서는 Structured Simplification에 해당하는 channel pruning을 제안하며, 마지막에 실험적으로 또 다른 structured simplification중 하나인 tensor factorization과, 논문에서 제안하는 channel pruning방식이 서로 complementary함을 보인다.
 
Main contribution/novelty: Channel pruning algorithm을 통해 2~5x speed 향상을 얻으며, 에러는 negligible 하게 (~1%) 오름.
Classification과 Detection pipeline에 둘 다 실험적으로 적용한 내용이 포함되어있다.
 
Proposed method: Two-step approach
	
* Channel selection (using Lasso regression), L1 (L0 is NP-hard)
    * prune할 feature map이 주어졌을 때, 중요한 channel들을 추출
	
* 추출된 (남은) channel들을 바탕으로 이후 feature map을 reconstruct하도록 optimize함.
    * (K-means와 비슷한 느낌의 iterative algorithm)

얼마나 pruning하는가?: 이는 target speedup에 따라서 설정해줄 수 있음. 많은 speedup을 원할수록 pruning되는 channel 또한 많아짐.
실험적으로 네트워크의 deeper layer들은 prune하기가 어렵고, shallow layer들은 prune하기가 비교적 쉬운것을 발견.
* *"More redundancy in shallow layers of the network"
따라서 shallow layer들에서 더 많은 pruning을 하는 방향으로 진행됨.
 
실험 결과 tensor factorization보다 근소하게 낮은 성능을 보임. 하지만 channel pruning은 tensor factorization과 다르게 "keeps the original model architecture, does not introduce additional layers, and the speed-up ratio on GPU is much higher" 라고 함.

+또한 Tensor factorization + Channel pruning으로 결국 가장 빠른 speedup 대비 낮은 성능 하락을 갱신함.
 
Writing에 오류가 많고, 깔끔하게 써져있지 않아서 막 편하게 읽을 수 있는 논문은 아니지만 efficient (memory/ time) network에 관심이 있다면 어렵지 않게 읽어볼만 한 논문