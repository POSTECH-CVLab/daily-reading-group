# Baking Neural Radiance Fields for Real-Time View Synthesis 
### Peter Hedman et al., Google Research
#### Summarized by Jinoh Cho

* 기존의 NeRF는 Inference Time때에 각 ray에 대해서 (Novel View Image의 픽셀 값을 생성하기 위해서) MLP를 일반적으로 수백번(ray가 지나가는 점들을 샘플링 한 수 만큼)을 통과 해야 되기 때문에 대충 single frame rendering에 대략 1분 정도의 시간이 필요하다는 단점을 가지고 있어 VR과 AR과 같은 real time application에 적용이 불가하다는 단점이 있음. 이를 해결하려는 논문이다.
* 이를 해결하기 위해서 3가지 방안을 통해서 해결하고자 한다. 첫번째, Nerf의 최대 장점 중 하나인 View Dependent Effect를 ray가 지나가는 sample point마다 계산하는 것이 아닌 ray마다 한번 View Dependent Effect를 계산하는 방법으로 바꾼다. 두번째, 기존의 NeRF는 Inference Time에 Opacity와 Color를  각 포인트마다 계산하는데 이를 미리 계산하고 값을 저장해 놓는 방법을 통해서 시간을 단축하고자 한다. 또한 값을 효과적으로 8bit integer값으로 저장할 수 있는 방법을 제안 한다. 세번째, Rendering 시간과 저장 공간의 크기가 sparsity of opacity에 의존적이기 때문에 우리는 Opacity Regularization Loss를 추가한다. 
* 위에서 이야기한 첫번째 방법을 조금 더 구체적으로 살펴보면, 기존의 Nerf는 단순하게 각 샘플링 포인트와 viewing direction을 인풋으로 받아 Opacity와 Color 값을 내뱉게 된다. Rendering시에 사이즈가 큰 MLP를 각 ray당 수백번 통과 해야된다는 단점이 있음. 하지만 이 논문에서 제안한 방법은  viewing direction없이 좌표 값을 인풋으로 받아 dencity, color, feature vector를 내뱉게 되고 이 값을 accumulate하여 미리 계산하여 특별한 Data structure에 저장해둔다. 그리고 나중에 렌더링 시에 ray당 view dependent 특징을 살리기 위해 accumulated feature vector와 ray의 viewing direction을 인풋으로 넣어 한번만 MLP 레이어를 통과 시켜주는 작업을 통해서 view dependent 특징을 살려 주게 된다. 이것이 큰 차이를 만들게 된다.
* SNeRG의 데이터 구조는 아직 정확히 이해를 하지 못해서 업데이트 예정



