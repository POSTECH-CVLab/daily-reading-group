# A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications
### Yun-Hao Cao et al., Nanjing University
#### Summarized by Woohyeon Sim

* **CNN에 '학습 없이 초기화한 것 만으로 object를 localize할 수 있다'는 inductive bias가 있음을 주장함. 그리고 이 성질을 이용해 foreground와 background를 분리하고 다른 이미지와 섞는 augmentation을 제안하여 self-supervised learning에서의 효과를 보여준 논문.**
	
* **Object localization**: GAP 직전 feature map을 가져와 channel쪽으로 sum한뒤, spatial median값으로 thresholding하여 남아있는 걸 foreground, 지워진걸 background로 설정. 동일한 과정을 다른 논문은 pre-trained weight을 쓰고 mean값으로 thresholding 하고 localization하는 걸 보여줬는데 결과는 크게 차이가 나지 않는다고함. 따라서 저자는 본 과정이 unsupervised로 진행할수 있으므로 augmentation으로 활용하여 self-supervised learning에 적용하고자함.
	
* **Self-supervised learning**: 한 이미지에 두 view를 만드는 과정에서, 하나는 기존 augmentation을 그대로 쓰고, 남은 하나는 확률 p로 제안한 것을 적용할지 말지를 정함. 즉, p가 0이면 기존 방법과 동일하게됨. augmentation 과정은 foreground는 놔두고 background만 다른 이미지에서 가져오는 식이고 background는 랜덤 배치함. median으로 thresholding한 이유는 fg:bg가 0.5:0.5가 되기 위함이고 이것으로 background를 어느 이미지에서 가져오든 상관없게 하였음.
	
* **주요 실험 결과**: 1) 일반적으로 쓰는 random init은 다 잘됨 std.도 작음, 2) random init.으로 CAM을 구하면 localize가 잘 안됨, 3) ReLU와 max pooling이 적을 때 localize 성능이 낮음. ReLU와 max pooling이 textureless한 부분을 deactivate시켜서 background를 지워주는 역할을 하는데 이 기능을 못하게 되니 그런거라고 함. 더불어 ReLU와 maxpooling이 inductive bias를 만드는 핵심적인 모듈이 아닐까라고 추론함. 4) self-sup learning의 downstream task (classification transfer learning, detection, segmentation)에서 의미있는 성능 향상인지는 모르겠지만 성능이 향상됨, 5) 다른 augmentation과의 의존성이 낮음. 다른 augmentation이 없어도 성능이 많이 떨어지지 않음.
	
* **총평**: 학습 없이 random init.만으로 localization할 수 있다는 것이 신기함. 그리고 그것을 응용할 수 있는 문제를 잘 찾은 것 같음. 여러 테스크와 셋팅에서 실험하여 그 효과를 보이려고 했음. 단, 아쉬운 점은 모든 실험을 ResNet기반으로 4x4 size에서 localize와 mixup을 하였는데 그것에 관한 언급이나 관련 실험이 없다는 것임.



