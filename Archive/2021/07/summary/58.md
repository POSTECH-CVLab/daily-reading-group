# Big self-supervised Models are Strong Semi-supervised learners
### Neurips 2020
#### Summarized by Suhyun Jung
---

**Overall** : 
동일한 개수의 labeled image 로 학습된 supervised 모델보다 높은 성능을 내어 image 도메인 에서 self-supervised pretraining + supervised finetuning 의 효과를 보였다. 
simCLR (A Simple Framework for Contrastive Learning of Visual Representations ) 에서 bigger model 을 사용할 수록 성능이 더 높아졌던 결과가 있었는데, 이를 이용하여 본 논문에서는 다음의 세 단계로 이루어지는 semi-supervised learning algorithm 을 제안하였다. : unsupervised pretraining with big model(using SimCLR) -> supervised fine tuning -> distillation with unlabeled examples 

 

**task** : 2d image classification / semi-supervised learning

 

**method** :

**unsupervised training with big model**:  simCLR 의 method 를 이용하여 semi-supervised learning 을 진행하였다. simCLR 과 비교하여 달라진 점은 larger ResNet model 을 사용하고(Resnet-50 -> Resnet-152+(3x wider channel + selective kernel)), deeper projection head 를 사용한 것이다.(2 layer -> 3 layer) encoder에서 layer 수는 늘릴 수록 높은 성능이 관측되었으나(152 까지는), width 의 경우 Resnet-152(2x +SK) 와 Resnet-152(3x + SK)의 성능 차이가 적었던 것으로 보아 더 늘리는 것이 효과가 없을 것이라고 시사했다.

 

**supervised fine-tuning** : unlabeled data 로 pretraining 한 것을 finetuning 할 때, 3-layer head 의 경우 head 의 first layer output 를 input 으로 하여 finetuning 을 진행하는 것이 일반적인 finetuning (head 의 input 을 input 으로 사용) 보다 성능이 더 높았다. 

 

**distillation with unlabled examples** : unlabeled data 를 이용해 distillation 을 진행한다. labeled data 를 동시 사용하여 distillation loss 와 labeled loss 를 적절히 가중치를 두어 distillation 을 진행하기도 했는데, unlabeled data 를 사용하는 경우와 비교해 성능 차이가 작아서 simplicity 를 위해 unlabled data만 사용했다고 함. best performance 를 위해 big model 이 self-distillation 을 진행한 뒤 small model 로 distillation 을 진행했다.

 

**총평** :100% labeled set 보다 10% labeled set + 90% unlabeled set 이 accuracy 가 더 높은 점이 인상 깊었습니다.  결과를 보면 Resnet-50 으로 학습된 simCLR2 도 resnet-50 으로 100% supervised learning 한 경우보다 accuracy 가 높던데 적절한 비율의 unlabeled image 로 인한 generalized data 생성 + 3 layer head 의 first layer output 을 finetuning 에 사용한 것이 생각보다 시너지 효과가 컸나 봅니다. 짧은 식견이지만 first layer output 을 사용하는 게 더 성능이 좋았던 이유는 local 한 feature 들을 보는 cnn based encoder 와는 달리, MLP 로 한 번 전체 feature 를 보면서 unlabeled data 로 학습된 generalized feature 들을 더 좋은 representation 으로 가공한 게 아닌가 싶네요.