# Rethinking and Improving the Robustness of Image Style Transfer
### Pei Wang et al., UC SAn Diego, Adobe Research - CVPR 2021 Oral
#### Summarized by Seungwook Kim

**Task**: Image Style Transfer improvement
 
**Main Motivation**:
ResNet의 등장 이후 VGG-style network보다 ResNet-style network들이 더 널리 사용되기 시작했지만 (Deeper + No dregradation), **Style transfer 분야에서는 VGG-style network가 항상 더 robust해왔음**.
오히려 ResNet-style network를 사용하여 style transfer를 하고자 하면 좋지 않은 결과 (degrades significantly)를 얻어왔음.
이 논문에서는 이 현상에 대한 이유와, 분석, 및 해결방법을 제안.
 
**Preliminary**:
Style transfer는, 이미지의 content를 유지시키기 위한 content loss와, style transfer를 위한 style loss가 있음.
여기서 Style은 수치적으로 Gram Matrix를 통해 나타냄.
**Gram Matrix**: "measure of similarity of the vectors of activations across the channel depth dimension" 
 
**ResNet에서 style transfer가 잘 안 되는 이유:**
VGG와 구조적 차이 (residual connection, 3x3 conv 보다 큰 conv, 1x1 conv, stride, maxpooling 등) 을 하나하나 비교해보았을 때,
**Residual connection가 style transfer에 가장 큰 악영향을 미치는 것을 파악함**
 
**Residual Connection이 안 좋은 이유: **
ResNet에서의 Gram Matrix는 Layer가 깊어질수록 max value가 높아지고, entropy가 낮아짐 (peaky)
--> 후반부에는 activation들이 single feature channel에 dominate되며, deterministic correlation pattern을 지님
VGG에서는 Layer가 깊어질수록 Max value가 낮아지고, entropy가 ResNet에 비해 높게, 일정하게 유지됨
 
**Max value가 높고, entropy가 낮은 peaky gram matrix가 style transfer에 안 좋은 이유:**
--> "THe optimization overfits on a few style patterns, ignoring most of the remaining"
--> 따라서 shallow layer에서의 style trasnfer는 나름 잘 되지만 (color pattern 등), deeper layer에서의 style transfer는 잘 안되는 것 (texture/style perception)
 
**Residual Connection을 사용하면 Gram matrix entropy가 낮아지고, Max value가 높아지는 이유:**
Bottleneck layer의 구조를 보면, residual connection은 이전 bottleneck의 ReLU "후"에서 시작해서 이번 bottleneck의 ReLU '전'으로 이어짐.
ReLU후에서 시작하기 때문에 ( >0), Residual connection이 낮아지기 위해서는 bottleneck layer내부적으로 (다음 ReLU 전) 억지로 activation이 낮아져야함
--> 하지만 그러면 원래 activation이 낮았던 부분들에서 그에 반발하여 activation이 커지게 됨
따라서 residual connection을 사용하는 경우, 한 번 큰 activation이 나타나면 그 이후로 계속 그와 동등하거나 이상의 값을 가진 activation이 나타날 수 밖에 없음 (구조적으로)
 
**이를 해결하는 방법 1:**
Residual connection이 이전 layer의 ReLU "전"에 시작하게 하는 방법
--> Style transfer에는 robust해졌지만, Image classification 성능이 너무 낮아짐 (77% -> 65%).
 
**이를 해결하는 방법 2: **
Knowledge Distillation motivation: Hard target보다 Soft target이 더 학습이 도움이 된다는 observation (soft targets provide gradients with higher variance).
따라서 style transfer에 사용되는 content loss와 style loss를 smooth시켜주면, gram matrix가 peaky + low entropy여도 학습이 잘 될 것이라는 motivation!
따라서 Content Loss와 Style loss에 softmax를 취해서 smoothing해줌 (Stylization with Activation SmoothinG, SWAG)
이 간단한 방법으로 Residual Connection을 사용하면서도 style trasnfer를 잘 할 수 있게 되는 robust한 loss function을 제안.
Qualitative/Quantitative(Amazon Mechanical Turk)적으로 성능이 좋아짐을 보임.
 
**총평**)


	
* 읽기 매우 편하게 되어있습니다. 기본적인 Style Transfer formulation도 잘 나와있어서 초심자도 읽기 편할 듯 합니다.
	
* 언뜻 보기에 너무나도 간단한 방법으로 문제를 해결해서 technical novelty가 부족해보일 수 있지만, VGG와 ResNet의 어떤 부분에서 차이가 발생하는지, 그 차이가 왜 발생하는지, 어떻게 해결하는지를 매우 깔끔해서 분석해서 Oral Paper로 accept된 건 아닐까 생각해봅니다.
	
* 새로운 문제를 푸는 것만이 아니라, 존재하지만 사람들이 크게 문제 삼지 않는 부분을 꼬집는 방향으로도 좋은 논문을 낼 수 있는 것 같습니다