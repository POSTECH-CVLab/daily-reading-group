# Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers
### Hila Chefer et al., Tel Aviv University, FAIR - ICCV 2021 Oral
#### Summarized by Seungwook Kim
---

**Task**: \
Explain prediction by any transformer-based architecture \
Including bi-modal transformers and transformers with co-attention
 
**Main contribution:** \
First explainability method appicable to ALL transformer architectures \
(Pure self attention, self-attention + co-attention, encoder-decoder attention)
 
Easier to implement than existing methods (예: 어제 읽은 논문)
 
Superior compred to existing methods
 
**Vs. Previous works:**
 
Vs. directly employing attention maps \
--> neglects intermediate attention scores  \
--> neglects other components (ex. residual connection) of transformers
 
Vs. LRP \
--> Does not propagate the relevancy scores back to the input to produce a heatmap.
 
Vs. Attention rollout \
THis method fails to distinguish positive/negative contributions \
--> leads to accumulation of relevancy scores even when they should be cancelled out
 
Vs. Attention flow \
--> Too slow to support large-scale evaluations|
 
Vs. CVPR2021 work (어제 읽은 논문, transformer interpretability beyond...) \
--> Only applicable for self-attention modules, not for other types of transformers
 
**Method**: Relevancy propagation - "how relevant is this input w.r.t certain output" \
Proposes relevancy propagation rules for BOTH self-attention interactions and cross-attention interactions \
(image-image, text-text, text-image, image-text) \
--> Relevancy propagation rules : 간단한 수식을 통해 전개: \
Multi-head Self attention에서 head별 contribution을 별개로 고려 (gradient 값 사용), \
이를 바탕으로 self-attention과 cross-attention interaction에 대한 propagation rule을 제안.
 
CVPR2021work와 마찬가지로 positive 값만 고려 (RELU를 사용한 것 처럼) \
Relevancy initialization: self-attention의 경우 Identity matrix, cross-attention의 경우 0 (각 modality는 다른 modality에 대한 context가 없으므로)
 
> Note: Does not use LRP (used in CVPR2021), empirically shows that it is not necessary -> simpler implementation!
 
**Results:** \
Qualitatively outperforms other methods, \
SoTA on positive/negative pertubation tests \
--> Pertubation test: Relavance가 높은순/낮은순으로 데이터를 제거했을 때 성능이 얼마나 하락하는지/유지되는지 \
--> 높은순 데이터를 제거했을 때 성능이 더 하락할수록 "explain/visualize"가 잘 된 것
SoTA as a weakly supervised segmentation method against other visualization methods.
 
**총평:**
어제 읽었던 논문보다도 훨씬 간단하고 (LRP사용 X), 수학적으로도 이해하기 쉬우며, 다양한 transformer architecture에 적용 가능한 방식입니다. \
아마 그런 이유로 Oral로 accept된 것 같기도 합니다. \
Formulation은 굳이 첨부하지 않았지만 찾아봐서 읽어보신다면 바로 이해가 될 정도입니다.
 
중요한 부분을 재차 강조하자면, 지금까지 transformer를 시각화할 때는 단순히 attention map을 사용했는데, 이는 intermediate attention score과 transformer의 다른 component를 고려하지 않은 방법입니다.  \
따라서 다른 많은 방법들이 제안되었지만, 본 논문에서 제안된 방법이 가장 널리 applicable하며 성능도 좋은 것으로 보입니다. \
novelty측면에서 크게 뛰어나지는 않지만, 사용성 측면세는 괜찮은 논문이라고 생각합니다.